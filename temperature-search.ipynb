{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-18T14:31:07.391537Z","iopub.execute_input":"2023-11-18T14:31:07.392249Z","iopub.status.idle":"2023-11-18T14:31:07.734118Z","shell.execute_reply.started":"2023-11-18T14:31:07.392192Z","shell.execute_reply":"2023-11-18T14:31:07.733196Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom scipy import ndimage\nimport math\nimport ast #to easily read out class text file that contains some unknwn syntax.\nimport scipy   #to upscale the image\nimport matplotlib.pyplot as plt\nimport cv2     \nfrom keras.applications.resnet import ResNet50, preprocess_input\nfrom keras.models import Model   \nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:31:08.976802Z","iopub.execute_input":"2023-11-18T14:31:08.977541Z","iopub.status.idle":"2023-11-18T14:31:19.709070Z","shell.execute_reply.started":"2023-11-18T14:31:08.977506Z","shell.execute_reply":"2023-11-18T14:31:19.708273Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"(training_images, training_labels) , (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n\ndef preprocess_image_input(input_images):\n  input_images = input_images.astype('float32')\n  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n  return output_ims\n\ntrain_X = preprocess_image_input(training_images)\ntest_X = preprocess_image_input(test_images)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:31:19.711008Z","iopub.execute_input":"2023-11-18T14:31:19.711689Z","iopub.status.idle":"2023-11-18T14:31:27.462234Z","shell.execute_reply.started":"2023-11-18T14:31:19.711653Z","shell.execute_reply":"2023-11-18T14:31:27.461272Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 5s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"def feature_extractor(inputs):\n\n  feature_extractor = tf.keras.applications.resnet.ResNet50(input_shape=(224, 224, 3),\n                                               include_top=False,\n                                               weights='imagenet')(inputs)\n \n  return feature_extractor\n\n\n\ndef classifier(inputs):\n    x= tf.keras.layers.Conv2D(2048, (1, 1), strides=(1, 1), padding=\"same\")(inputs)\n    x= tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)  \n    x = tf.keras.layers.Dense(10,name=\"classification\")(x)\n    return x\n\n\n\ndef final_model(inputs):\n\n    resize = tf.keras.layers.UpSampling2D(size=(7,7))(inputs)\n\n    resnet_feature_extractor = feature_extractor(resize)\n    \n    \n    classification_output = classifier(resnet_feature_extractor)\n\n    return classification_output\n\n\n\n\ndef define_compile_model():\n  inputs = tf.keras.layers.Input(shape=(32,32,3))\n  \n  classification_output = final_model(inputs) \n  model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n \n  model.compile(optimizer='SGD', \n                loss='sparse_categorical_crossentropy',\n                metrics = ['accuracy'])\n  \n  return model\n\n\nresnet50 = define_compile_model()\n\nresnet50.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:31:27.463479Z","iopub.execute_input":"2023-11-18T14:31:27.463845Z","iopub.status.idle":"2023-11-18T14:31:33.467325Z","shell.execute_reply.started":"2023-11-18T14:31:27.463810Z","shell.execute_reply":"2023-11-18T14:31:33.466384Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 0s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n up_sampling2d (UpSampling2  (None, 224, 224, 3)       0         \n D)                                                              \n                                                                 \n resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n                                                                 \n conv2d (Conv2D)             (None, 7, 7, 2048)        4196352   \n                                                                 \n re_lu (ReLU)                (None, 7, 7, 2048)        0         \n                                                                 \n global_average_pooling2d (  (None, 2048)              0         \n GlobalAveragePooling2D)                                         \n                                                                 \n classification (Dense)      (None, 10)                20490     \n                                                                 \n=================================================================\nTotal params: 27804554 (106.07 MB)\nTrainable params: 27751434 (105.86 MB)\nNon-trainable params: 53120 (207.50 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"resnet50.compile(\n    optimizer=keras.optimizers.SGD(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:31:33.469097Z","iopub.execute_input":"2023-11-18T14:31:33.469395Z","iopub.status.idle":"2023-11-18T14:31:33.487311Z","shell.execute_reply.started":"2023-11-18T14:31:33.469368Z","shell.execute_reply":"2023-11-18T14:31:33.486523Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"resnet50.fit(train_X, training_labels, epochs=3, validation_data = (test_X, test_labels), batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:31:33.488516Z","iopub.execute_input":"2023-11-18T14:31:33.488858Z","iopub.status.idle":"2023-11-18T14:45:40.095747Z","shell.execute_reply.started":"2023-11-18T14:31:33.488823Z","shell.execute_reply":"2023-11-18T14:45:40.094864Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/3\n782/782 [==============================] - 300s 352ms/step - loss: 0.3994 - accuracy: 0.8683 - val_loss: 0.2175 - val_accuracy: 0.9285\nEpoch 2/3\n782/782 [==============================] - 272s 348ms/step - loss: 0.1067 - accuracy: 0.9645 - val_loss: 0.1724 - val_accuracy: 0.9411\nEpoch 3/3\n782/782 [==============================] - 272s 348ms/step - loss: 0.0397 - accuracy: 0.9890 - val_loss: 0.1708 - val_accuracy: 0.9468\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x78c7f022b880>"},"metadata":{}}]},{"cell_type":"code","source":"#proportionality maintained student\nstudent_bl = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10),\n    ],\n    name=\"student_bl\",\n)\n\nstudent_bl.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:30:11.387671Z","iopub.execute_input":"2023-11-18T17:30:11.388049Z","iopub.status.idle":"2023-11-18T17:30:11.482825Z","shell.execute_reply.started":"2023-11-18T17:30:11.388021Z","shell.execute_reply":"2023-11-18T17:30:11.481926Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Model: \"student_bl\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_28 (Conv2D)          (None, 16, 16, 128)       3584      \n                                                                 \n leaky_re_lu_18 (LeakyReLU)  (None, 16, 16, 128)       0         \n                                                                 \n max_pooling2d_18 (MaxPooli  (None, 16, 16, 128)       0         \n ng2D)                                                           \n                                                                 \n conv2d_29 (Conv2D)          (None, 8, 8, 64)          73792     \n                                                                 \n leaky_re_lu_19 (LeakyReLU)  (None, 8, 8, 64)          0         \n                                                                 \n max_pooling2d_19 (MaxPooli  (None, 8, 8, 64)          0         \n ng2D)                                                           \n                                                                 \n conv2d_30 (Conv2D)          (None, 4, 4, 32)          18464     \n                                                                 \n global_average_pooling2d_1  (None, 32)                0         \n 0 (GlobalAveragePooling2D)                                      \n                                                                 \n dense_9 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 96170 (375.66 KB)\nTrainable params: 96170 (375.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=2,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T14:48:23.373824Z","iopub.execute_input":"2023-11-18T14:48:23.374553Z","iopub.status.idle":"2023-11-18T15:03:19.167601Z","shell.execute_reply.started":"2023-11-18T14:48:23.374518Z","shell.execute_reply":"2023-11-18T15:03:19.166740Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 93s 57ms/step - accuracy: 0.4040 - student_loss: 2.3762 - distillation_loss: 6.0535\nEpoch 2/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.5167 - student_loss: 1.6170 - distillation_loss: 4.5571\nEpoch 3/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.5661 - student_loss: 1.4672 - distillation_loss: 4.1187\nEpoch 4/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6001 - student_loss: 1.3370 - distillation_loss: 3.7718\nEpoch 5/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6217 - student_loss: 1.2556 - distillation_loss: 3.5563\nEpoch 6/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6445 - student_loss: 1.1981 - distillation_loss: 3.3923\nEpoch 7/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6534 - student_loss: 1.1545 - distillation_loss: 3.2649\nEpoch 8/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6687 - student_loss: 1.1020 - distillation_loss: 3.1223\nEpoch 9/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6762 - student_loss: 1.0846 - distillation_loss: 3.0613\nEpoch 10/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6841 - student_loss: 1.0537 - distillation_loss: 2.9755\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6680 - student_loss: 1.0824\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[0.6679999828338623, 1.0420880317687988]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=3,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:13:02.404070Z","iopub.execute_input":"2023-11-18T17:13:02.404947Z","iopub.status.idle":"2023-11-18T17:27:52.556502Z","shell.execute_reply.started":"2023-11-18T17:13:02.404909Z","shell.execute_reply":"2023-11-18T17:27:52.555180Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 91s 56ms/step - accuracy: 0.4068 - student_loss: 2.2418 - distillation_loss: 9.6235\nEpoch 2/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.5278 - student_loss: 1.7327 - distillation_loss: 7.4847\nEpoch 3/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.5772 - student_loss: 1.5359 - distillation_loss: 6.7208\nEpoch 4/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6079 - student_loss: 1.4075 - distillation_loss: 6.2179\nEpoch 5/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6308 - student_loss: 1.3231 - distillation_loss: 5.8910\nEpoch 6/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6498 - student_loss: 1.2542 - distillation_loss: 5.6376\nEpoch 7/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6625 - student_loss: 1.2005 - distillation_loss: 5.4065\nEpoch 8/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6729 - student_loss: 1.1634 - distillation_loss: 5.2468\nEpoch 9/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6820 - student_loss: 1.1184 - distillation_loss: 5.0893\nEpoch 10/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6919 - student_loss: 1.0836 - distillation_loss: 4.9695\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6715 - student_loss: 1.2583\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[0.671500027179718, 1.1355122327804565]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=4,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:04:35.197766Z","iopub.execute_input":"2023-11-18T15:04:35.198433Z","iopub.status.idle":"2023-11-18T15:19:20.939570Z","shell.execute_reply.started":"2023-11-18T15:04:35.198398Z","shell.execute_reply":"2023-11-18T15:19:20.938715Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 91s 56ms/step - accuracy: 0.4124 - student_loss: 2.2044 - distillation_loss: 11.5461\nEpoch 2/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.5317 - student_loss: 1.7269 - distillation_loss: 9.1594\nEpoch 3/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.5750 - student_loss: 1.5500 - distillation_loss: 8.3771\nEpoch 4/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6053 - student_loss: 1.4230 - distillation_loss: 7.8346\nEpoch 5/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6328 - student_loss: 1.3222 - distillation_loss: 7.3896\nEpoch 6/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6459 - student_loss: 1.2624 - distillation_loss: 7.0884\nEpoch 7/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6610 - student_loss: 1.2117 - distillation_loss: 6.8653\nEpoch 8/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6702 - student_loss: 1.1702 - distillation_loss: 6.6557\nEpoch 9/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6818 - student_loss: 1.1254 - distillation_loss: 6.4639\nEpoch 10/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6880 - student_loss: 1.0972 - distillation_loss: 6.3258\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6491 - student_loss: 1.2989\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[0.6491000056266785, 1.3188858032226562]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=5,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:19:40.516105Z","iopub.execute_input":"2023-11-18T15:19:40.516803Z","iopub.status.idle":"2023-11-18T15:34:29.119719Z","shell.execute_reply.started":"2023-11-18T15:19:40.516767Z","shell.execute_reply":"2023-11-18T15:34:29.118883Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 91s 56ms/step - accuracy: 0.4069 - student_loss: 2.2010 - distillation_loss: 12.4117\nEpoch 2/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.5253 - student_loss: 1.7435 - distillation_loss: 9.9990\nEpoch 3/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.5775 - student_loss: 1.5400 - distillation_loss: 9.0779\nEpoch 4/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6092 - student_loss: 1.4032 - distillation_loss: 8.4486\nEpoch 5/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6311 - student_loss: 1.3195 - distillation_loss: 8.0501\nEpoch 6/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6477 - student_loss: 1.2529 - distillation_loss: 7.7201\nEpoch 7/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6617 - student_loss: 1.2004 - distillation_loss: 7.4374\nEpoch 8/10\n1563/1563 [==============================] - 88s 56ms/step - accuracy: 0.6730 - student_loss: 1.1581 - distillation_loss: 7.2338\nEpoch 9/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6806 - student_loss: 1.1247 - distillation_loss: 7.0568\nEpoch 10/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6875 - student_loss: 1.1000 - distillation_loss: 6.9201\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6526 - student_loss: 1.2318\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[0.6525999903678894, 1.1079679727554321]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=6,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T15:44:44.207189Z","iopub.execute_input":"2023-11-18T15:44:44.207570Z","iopub.status.idle":"2023-11-18T15:59:52.568894Z","shell.execute_reply.started":"2023-11-18T15:44:44.207540Z","shell.execute_reply":"2023-11-18T15:59:52.567998Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 93s 57ms/step - accuracy: 0.4169 - student_loss: 2.1568 - distillation_loss: 12.3398\nEpoch 2/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.5330 - student_loss: 1.6872 - distillation_loss: 9.9787\nEpoch 3/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.5835 - student_loss: 1.4958 - distillation_loss: 9.1248\nEpoch 4/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.6114 - student_loss: 1.3833 - distillation_loss: 8.6097\nEpoch 5/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.6322 - student_loss: 1.3019 - distillation_loss: 8.2210\nEpoch 6/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.6459 - student_loss: 1.2424 - distillation_loss: 7.8918\nEpoch 7/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.6618 - student_loss: 1.1852 - distillation_loss: 7.6287\nEpoch 8/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6709 - student_loss: 1.1501 - distillation_loss: 7.4379\nEpoch 9/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.6761 - student_loss: 1.1269 - distillation_loss: 7.2685\nEpoch 10/10\n1563/1563 [==============================] - 90s 58ms/step - accuracy: 0.6878 - student_loss: 1.0771 - distillation_loss: 7.0656\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6661 - student_loss: 1.2037\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.666100025177002, 1.7322781085968018]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=7,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T16:07:35.357544Z","iopub.execute_input":"2023-11-18T16:07:35.357904Z","iopub.status.idle":"2023-11-18T16:22:32.440805Z","shell.execute_reply.started":"2023-11-18T16:07:35.357874Z","shell.execute_reply":"2023-11-18T16:22:32.439990Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 92s 57ms/step - accuracy: 0.4137 - student_loss: 2.1461 - distillation_loss: 12.1106\nEpoch 2/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.5265 - student_loss: 1.6790 - distillation_loss: 9.8193\nEpoch 3/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.5770 - student_loss: 1.4979 - distillation_loss: 9.0466\nEpoch 4/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6061 - student_loss: 1.3782 - distillation_loss: 8.5298\nEpoch 5/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6251 - student_loss: 1.3056 - distillation_loss: 8.1815\nEpoch 6/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6429 - student_loss: 1.2341 - distillation_loss: 7.8474\nEpoch 7/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6550 - student_loss: 1.1953 - distillation_loss: 7.6258\nEpoch 8/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6679 - student_loss: 1.1415 - distillation_loss: 7.3702\nEpoch 9/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6767 - student_loss: 1.1098 - distillation_loss: 7.1994\nEpoch 10/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6849 - student_loss: 1.0778 - distillation_loss: 7.0375\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6625 - student_loss: 1.1881\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[0.6625000238418579, 1.2223527431488037]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=8,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T16:27:34.593072Z","iopub.execute_input":"2023-11-18T16:27:34.593678Z","iopub.status.idle":"2023-11-18T16:42:25.585462Z","shell.execute_reply.started":"2023-11-18T16:27:34.593642Z","shell.execute_reply":"2023-11-18T16:42:25.584559Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 92s 57ms/step - accuracy: 0.4190 - student_loss: 2.0823 - distillation_loss: 11.6396\nEpoch 2/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.5303 - student_loss: 1.6585 - distillation_loss: 9.5960\nEpoch 3/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.5778 - student_loss: 1.4711 - distillation_loss: 8.8203\nEpoch 4/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6051 - student_loss: 1.3586 - distillation_loss: 8.3212\nEpoch 5/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6277 - student_loss: 1.2819 - distillation_loss: 7.9419\nEpoch 6/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6462 - student_loss: 1.2117 - distillation_loss: 7.6341\nEpoch 7/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6568 - student_loss: 1.1675 - distillation_loss: 7.3947\nEpoch 8/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6660 - student_loss: 1.1314 - distillation_loss: 7.2355\nEpoch 9/10\n1563/1563 [==============================] - 89s 57ms/step - accuracy: 0.6765 - student_loss: 1.0949 - distillation_loss: 7.0385\nEpoch 10/10\n1563/1563 [==============================] - 88s 57ms/step - accuracy: 0.6837 - student_loss: 1.0679 - distillation_loss: 6.8994\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6530 - student_loss: 1.2175\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[0.652999997138977, 1.3971123695373535]"},"metadata":{}}]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n\ndistiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=9,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:30:18.490845Z","iopub.execute_input":"2023-11-18T17:30:18.491229Z","iopub.status.idle":"2023-11-18T17:45:22.064989Z","shell.execute_reply.started":"2023-11-18T17:30:18.491185Z","shell.execute_reply":"2023-11-18T17:45:22.064094Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/10\n1563/1563 [==============================] - 93s 57ms/step - accuracy: 0.4141 - student_loss: 2.0653 - distillation_loss: 11.2136\nEpoch 2/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.5319 - student_loss: 1.6219 - distillation_loss: 9.2394\nEpoch 3/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.5812 - student_loss: 1.4391 - distillation_loss: 8.4922\nEpoch 4/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6132 - student_loss: 1.3175 - distillation_loss: 7.9715\nEpoch 5/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6328 - student_loss: 1.2403 - distillation_loss: 7.6298\nEpoch 6/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6483 - student_loss: 1.1825 - distillation_loss: 7.3349\nEpoch 7/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6617 - student_loss: 1.1409 - distillation_loss: 7.1254\nEpoch 8/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6710 - student_loss: 1.1080 - distillation_loss: 6.9461\nEpoch 9/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6806 - student_loss: 1.0693 - distillation_loss: 6.7661\nEpoch 10/10\n1563/1563 [==============================] - 90s 57ms/step - accuracy: 0.6857 - student_loss: 1.0530 - distillation_loss: 6.6661\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6535 - student_loss: 1.2356\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[0.6535000205039978, 0.8826004266738892]"},"metadata":{}}]}]}