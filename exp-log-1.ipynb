{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-15T07:56:53.002643Z","iopub.execute_input":"2023-11-15T07:56:53.002945Z","iopub.status.idle":"2023-11-15T07:56:53.356565Z","shell.execute_reply.started":"2023-11-15T07:56:53.002919Z","shell.execute_reply":"2023-11-15T07:56:53.355525Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nfrom scipy import ndimage\nimport math\nimport ast #to easily read out class text file that contains some unknwn syntax.\nimport scipy   #to upscale the image\nimport matplotlib.pyplot as plt\nimport cv2     \nfrom keras.applications.resnet import ResNet50, preprocess_input\nfrom keras.models import Model   \nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:56:53.358340Z","iopub.execute_input":"2023-11-15T07:56:53.358737Z","iopub.status.idle":"2023-11-15T07:57:04.621668Z","shell.execute_reply.started":"2023-11-15T07:56:53.358709Z","shell.execute_reply":"2023-11-15T07:57:04.620452Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load and preprocess CIFAR10 Dataset","metadata":{}},{"cell_type":"code","source":"(training_images, training_labels) , (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n\ndef preprocess_image_input(input_images):\n  input_images = input_images.astype('float32')\n  output_ims = tf.keras.applications.resnet50.preprocess_input(input_images)\n  return output_ims\n\ntrain_X = preprocess_image_input(training_images)\ntest_X = preprocess_image_input(test_images)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:57:04.622902Z","iopub.execute_input":"2023-11-15T07:57:04.623582Z","iopub.status.idle":"2023-11-15T07:57:27.556807Z","shell.execute_reply.started":"2023-11-15T07:57:04.623548Z","shell.execute_reply":"2023-11-15T07:57:27.555802Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 19s 0us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Teacher Section","metadata":{}},{"cell_type":"markdown","source":"## Teacher Model - Resnet50 with custom top","metadata":{}},{"cell_type":"code","source":"def feature_extractor(inputs):\n\n  feature_extractor = tf.keras.applications.resnet.ResNet50(input_shape=(224, 224, 3),\n                                               include_top=False,\n                                               weights='imagenet')(inputs)\n \n  return feature_extractor\n\n\n\ndef classifier(inputs):\n    x= tf.keras.layers.Conv2D(2048, (1, 1), strides=(1, 1), padding=\"same\")(inputs)\n    x= tf.keras.layers.ReLU()(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)  \n    x = tf.keras.layers.Dense(10,name=\"classification\")(x)\n    return x\n\n\n\ndef final_model(inputs):\n\n    resize = tf.keras.layers.UpSampling2D(size=(7,7))(inputs)\n\n    resnet_feature_extractor = feature_extractor(resize)\n    \n    \n    classification_output = classifier(resnet_feature_extractor)\n\n    return classification_output\n\n\n\n\ndef define_compile_model():\n  inputs = tf.keras.layers.Input(shape=(32,32,3))\n  \n  classification_output = final_model(inputs) \n  model = tf.keras.Model(inputs=inputs, outputs = classification_output)\n \n  model.compile(optimizer='SGD', \n                loss='sparse_categorical_crossentropy',\n                metrics = ['accuracy'])\n  \n  return model\n\n\nresnet50 = define_compile_model()\n\nresnet50.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:57:27.558557Z","iopub.execute_input":"2023-11-15T07:57:27.558880Z","iopub.status.idle":"2023-11-15T07:57:37.974515Z","shell.execute_reply.started":"2023-11-15T07:57:27.558854Z","shell.execute_reply":"2023-11-15T07:57:37.973623Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94765736/94765736 [==============================] - 5s 0us/step\nModel: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n                                                                 \n up_sampling2d (UpSampling2  (None, 224, 224, 3)       0         \n D)                                                              \n                                                                 \n resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n                                                                 \n conv2d (Conv2D)             (None, 7, 7, 2048)        4196352   \n                                                                 \n re_lu (ReLU)                (None, 7, 7, 2048)        0         \n                                                                 \n global_average_pooling2d (  (None, 2048)              0         \n GlobalAveragePooling2D)                                         \n                                                                 \n classification (Dense)      (None, 10)                20490     \n                                                                 \n=================================================================\nTotal params: 27804554 (106.07 MB)\nTrainable params: 27751434 (105.86 MB)\nNon-trainable params: 53120 (207.50 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"resnet50.compile(\n    optimizer=keras.optimizers.SGD(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:57:37.975703Z","iopub.execute_input":"2023-11-15T07:57:37.976030Z","iopub.status.idle":"2023-11-15T07:57:37.993275Z","shell.execute_reply.started":"2023-11-15T07:57:37.976002Z","shell.execute_reply":"2023-11-15T07:57:37.992404Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Teacher Training","metadata":{}},{"cell_type":"code","source":"resnet50.fit(train_X, training_labels, epochs=3, validation_data = (test_X, test_labels), batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:57:37.994401Z","iopub.execute_input":"2023-11-15T07:57:37.994832Z","iopub.status.idle":"2023-11-15T08:11:41.778418Z","shell.execute_reply.started":"2023-11-15T07:57:37.994801Z","shell.execute_reply":"2023-11-15T08:11:41.777387Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Epoch 1/3\n782/782 [==============================] - 298s 351ms/step - loss: 0.3963 - accuracy: 0.8689 - val_loss: 0.2066 - val_accuracy: 0.9313\nEpoch 2/3\n782/782 [==============================] - 271s 347ms/step - loss: 0.1077 - accuracy: 0.9650 - val_loss: 0.1820 - val_accuracy: 0.9414\nEpoch 3/3\n782/782 [==============================] - 272s 347ms/step - loss: 0.0395 - accuracy: 0.9891 - val_loss: 0.1933 - val_accuracy: 0.9405\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x7eadec557cd0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Baseline Distillation ","metadata":{}},{"cell_type":"code","source":"#proportionality maintained student\nstudent_bl = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10),\n    ],\n    name=\"student_bl\",\n)\n\nstudent_bl.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:28:30.700056Z","iopub.execute_input":"2023-11-15T07:28:30.700421Z","iopub.status.idle":"2023-11-15T07:28:30.791145Z","shell.execute_reply.started":"2023-11-15T07:28:30.700391Z","shell.execute_reply":"2023-11-15T07:28:30.790174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Distiller_bl(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n       \n        x, y = data\n\n  \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n           \n            student_predictions = self.student(x, training=True)\n\n            \n            student_loss = self.student_loss_fn(y, student_predictions)\n\n            \n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n       \n        x, y = data\n\n      \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:28:33.198285Z","iopub.execute_input":"2023-11-15T07:28:33.198975Z","iopub.status.idle":"2023-11-15T07:28:33.212135Z","shell.execute_reply.started":"2023-11-15T07:28:33.198938Z","shell.execute_reply":"2023-11-15T07:28:33.211205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"distiller = Distiller_bl(student=student_bl, teacher=resnet50)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=3,\n)\n\n\ndistiller.fit(train_X, training_labels, epochs=10)\n\n\ndistiller.evaluate(test_X, test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T07:28:37.120622Z","iopub.execute_input":"2023-11-15T07:28:37.120977Z","iopub.status.idle":"2023-11-15T07:43:14.972150Z","shell.execute_reply.started":"2023-11-15T07:28:37.120950Z","shell.execute_reply":"2023-11-15T07:43:14.971101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distillation with only CAM loss","metadata":{}},{"cell_type":"code","source":"batch_size = 32\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_X, training_labels))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T08:26:04.128889Z","iopub.execute_input":"2023-11-15T08:26:04.129268Z","iopub.status.idle":"2023-11-15T08:26:05.804735Z","shell.execute_reply.started":"2023-11-15T08:26:04.129237Z","shell.execute_reply":"2023-11-15T08:26:05.803896Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#proportionality maintained student\nstudent_c1 = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10),\n    ],\n    name=\"student_c1\",\n)\n\nstudent_c1.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T08:26:06.637864Z","iopub.execute_input":"2023-11-15T08:26:06.638198Z","iopub.status.idle":"2023-11-15T08:26:06.742295Z","shell.execute_reply.started":"2023-11-15T08:26:06.638172Z","shell.execute_reply":"2023-11-15T08:26:06.741380Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model: \"student_c1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 16, 16, 128)       3584      \n                                                                 \n leaky_re_lu (LeakyReLU)     (None, 16, 16, 128)       0         \n                                                                 \n max_pooling2d (MaxPooling2  (None, 16, 16, 128)       0         \n D)                                                              \n                                                                 \n conv2d_2 (Conv2D)           (None, 8, 8, 64)          73792     \n                                                                 \n leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n g2D)                                                            \n                                                                 \n conv2d_3 (Conv2D)           (None, 4, 4, 32)          18464     \n                                                                 \n global_average_pooling2d_1  (None, 32)                0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dense (Dense)               (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 96170 (375.66 KB)\nTrainable params: 96170 (375.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def plt_heatmap(img,heatmap,title):\n    \n    fig,ax= plt.subplots()\n    ax.imshow(img)\n    #ax.imshow(heatmap, cmap='jet', alpha=0.5)\n    ax.set_title(title) \n    plt.show()\n\n\n\ndef cam_loss(model_one,model_two,layer_one,layer_two,cam_img): \n    cifar10_classes = [\n        \n        'Airplane',\n        'Automobile',\n        'Bird',\n        'Cat',\n        'Deer',\n        'Dog',\n        'Frog',\n        'Horse',\n        'Ship',\n        'Truck'\n    ]\n    \n    img_tensor = np.expand_dims(cam_img, axis=0)\n    preprocessed_img = preprocess_input(img_tensor)\n    last_layer_weights = model_one.layers[-1].get_weights()[0]\n    vis_model = Model(inputs=model_one.input, outputs=(model_one.layers[layer_one].output,model_one.layers[-1].output))\n    with tf.GradientTape() as tape:\n        last_conv_output, pred_vec = vis_model(preprocessed_img,training=False)\n      \n        last_conv_output = np.squeeze(last_conv_output) \n      \n        pred = np.argmax(pred_vec)\n   \n    upsampled_last_conv_output = ndimage.zoom(last_conv_output, (4.57, 4.57, 1), order=1)\n    last_layer_weights_for_pred = last_layer_weights[:, pred]\n    heat_map = np.dot(upsampled_last_conv_output, last_layer_weights_for_pred) \n    flat=heat_map.flatten()\n    \n    #plt_heatmap(cam_img,heat_map,cifar10_classes[pred])\n       \n    \n    last_layer_weights_2 = model_two.layers[-1].get_weights()[0]\n    vis_model_2 = Model(inputs=model_two.input, outputs=(model_two.layers[layer_two].output,model_two.layers[-1].output))\n    \n    with tf.GradientTape() as tape:\n        last_conv_output_2, pred_vec_2 = vis_model_2(preprocessed_img,training=False)\n        last_conv_output_2 = np.squeeze(last_conv_output_2)         \n        pred_2 = np.argmax(pred_vec_2)\n        \n    h = int(cam_img.shape[0]/last_conv_output_2.shape[0])\n    w = int(cam_img.shape[1]/last_conv_output_2.shape[1])\n    upsampled_last_conv_output_2 = ndimage.zoom(last_conv_output_2, (h, w, 1), order=1) \n    last_layer_weights_for_pred_2 = last_layer_weights_2[:, pred]\n    heat_map_2 = np.dot(upsampled_last_conv_output_2, last_layer_weights_for_pred_2) \n    flat_2=heat_map_2.flatten()\n    \n        \n    #plt_heatmap(cam_img,heat_map_2,cifar10_classes[pred_2])\n    \n    \n    \n    absolute_differences = np.abs(flat - flat_2)\n\n    mae = np.mean(absolute_differences)\n    \n        \n    \n    return mae\n\n\n\n\n\nclass Distiller(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n        self.indexing=0\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n       \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n        \n    \n    def train_step(self, x,y,conts):\n       \n       \n        x_numpy=x.numpy()\n       \n             \n        # retrieve the cam loss\n        caml=(cam_loss(self.teacher,self.student,-4,-3,x_numpy[0]))\n    \n      \n                                  \n         \n    \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n   \n            student_predictions = self.student(x, training=True)\n\n           \n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n     \n     \n            loss = self.alpha * student_loss + (1 - self.alpha) * caml\n            \n           \n                            \n        \n        \n        self.indexing=self.indexing+10\n        if self.indexing>49500:\n            self.indexing=0\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n       \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n        \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n       \n        return results,distillation_loss,caml\n\n    def test_step(self, data):\n     \n        x, y = data\n\n       \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        print('hi')\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results","metadata":{"execution":{"iopub.status.busy":"2023-11-15T08:26:10.252141Z","iopub.execute_input":"2023-11-15T08:26:10.252825Z","iopub.status.idle":"2023-11-15T08:26:10.276768Z","shell.execute_reply.started":"2023-11-15T08:26:10.252788Z","shell.execute_reply":"2023-11-15T08:26:10.275775Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"distiller_c1 = Distiller(student=student_c1, teacher=resnet50)\ndistiller_c1.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=3,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T08:26:16.395888Z","iopub.execute_input":"2023-11-15T08:26:16.396251Z","iopub.status.idle":"2023-11-15T08:26:16.415969Z","shell.execute_reply.started":"2023-11-15T08:26:16.396224Z","shell.execute_reply":"2023-11-15T08:26:16.415223Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"conts=1\nepochs = 10\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n\n    \n\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n       \n       \n\n        history,l1,l2=distiller_c1.train_step(x_batch_train,y_batch_train,conts)\n        conts+=1\n    print(conts)\n    tf.print(l1)\n    print(l2)\n    for key, value in history.items():\n        print(key, tf.print(value))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T08:26:18.811806Z","iopub.execute_input":"2023-11-15T08:26:18.812531Z","iopub.status.idle":"2023-11-15T10:03:44.553471Z","shell.execute_reply.started":"2023-11-15T08:26:18.812497Z","shell.execute_reply":"2023-11-15T10:03:44.552629Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\nStart of epoch 0\n1564\n10.433486\n12.830579\n0.39264\naccuracy None\n1.55721092\nstudent_loss None\n10.433486\ndistillation_loss None\n\nStart of epoch 1\n3127\n9.27807426\n11.692741\n0.4465\naccuracy None\n1.18062067\nstudent_loss None\n9.27807426\ndistillation_loss None\n\nStart of epoch 2\n4690\n9.54609585\n13.128896\n0.48118\naccuracy None\n2.00735426\nstudent_loss None\n9.54609585\ndistillation_loss None\n\nStart of epoch 3\n6253\n8.57855606\n12.52506\n0.506185\naccuracy None\n1.55245376\nstudent_loss None\n8.57855606\ndistillation_loss None\n\nStart of epoch 4\n7816\n7.32777834\n12.49141\n0.52642\naccuracy None\n1.06652486\nstudent_loss None\n7.32777834\ndistillation_loss None\n\nStart of epoch 5\n9379\n8.28400707\n13.057808\n0.543326676\naccuracy None\n0.993043423\nstudent_loss None\n8.28400707\ndistillation_loss None\n\nStart of epoch 6\n10942\n5.82007504\n19.01378\n0.557391405\naccuracy None\n0.747252\nstudent_loss None\n5.82007504\ndistillation_loss None\n\nStart of epoch 7\n12505\n6.44685888\n9.628952\n0.569197476\naccuracy None\n0.989153922\nstudent_loss None\n6.44685888\ndistillation_loss None\n\nStart of epoch 8\n14068\n6.61245298\n13.240078\n0.57975775\naccuracy None\n1.12509072\nstudent_loss None\n6.61245298\ndistillation_loss None\n\nStart of epoch 9\n15631\n7.07548618\n24.895111\n0.588828\naccuracy None\n1.1568743\nstudent_loss None\n7.07548618\ndistillation_loss None\n","output_type":"stream"}]},{"cell_type":"code","source":"distiller_c1.evaluate(test_X,test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T10:10:16.554648Z","iopub.execute_input":"2023-11-15T10:10:16.555525Z","iopub.status.idle":"2023-11-15T10:10:18.031513Z","shell.execute_reply.started":"2023-11-15T10:10:16.555495Z","shell.execute_reply":"2023-11-15T10:10:18.030521Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"hi\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6452 - student_loss: 1.0436\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[0.6452000141143799, 1.2904012203216553]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Distillation with cam loss (softmax,mae)","metadata":{}},{"cell_type":"code","source":"#proportionality maintained student\nstudent_c2 = keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10),\n    ],\n    name=\"student_c2\",\n)\n\nstudent_c2.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T10:10:26.650446Z","iopub.execute_input":"2023-11-15T10:10:26.650832Z","iopub.status.idle":"2023-11-15T10:10:26.749213Z","shell.execute_reply.started":"2023-11-15T10:10:26.650802Z","shell.execute_reply":"2023-11-15T10:10:26.748273Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"student_c2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_4 (Conv2D)           (None, 16, 16, 128)       3584      \n                                                                 \n leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 16, 16, 128)       0         \n g2D)                                                            \n                                                                 \n conv2d_5 (Conv2D)           (None, 8, 8, 64)          73792     \n                                                                 \n leaky_re_lu_3 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n max_pooling2d_3 (MaxPoolin  (None, 8, 8, 64)          0         \n g2D)                                                            \n                                                                 \n conv2d_6 (Conv2D)           (None, 4, 4, 32)          18464     \n                                                                 \n global_average_pooling2d_2  (None, 32)                0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dense_1 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 96170 (375.66 KB)\nTrainable params: 96170 (375.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def softmax(x):\n    exp_x = np.exp(x - np.max(x))  \n    return exp_x / exp_x.sum(axis=0, keepdims=True)\n\ndef plt_heatmap(img,heatmap,title):\n    \n    fig,ax= plt.subplots()\n    ax.imshow(img)\n    #ax.imshow(heatmap, cmap='jet', alpha=0.5)\n    ax.set_title(title) \n    plt.show()\n\n\n\ndef cam_loss(model_one,model_two,layer_one,layer_two,cam_img): \n    cifar10_classes = [\n        \n        'Airplane',\n        'Automobile',\n        'Bird',\n        'Cat',\n        'Deer',\n        'Dog',\n        'Frog',\n        'Horse',\n        'Ship',\n        'Truck'\n    ]\n    \n    img_tensor = np.expand_dims(cam_img, axis=0)\n    preprocessed_img = preprocess_input(img_tensor)\n    last_layer_weights = model_one.layers[-1].get_weights()[0]\n    vis_model = Model(inputs=model_one.input, outputs=(model_one.layers[layer_one].output,model_one.layers[-1].output))\n    with tf.GradientTape() as tape:\n        last_conv_output, pred_vec = vis_model(preprocessed_img,training=False)\n      \n        last_conv_output = np.squeeze(last_conv_output) \n      \n        pred = np.argmax(pred_vec)\n   \n    upsampled_last_conv_output = ndimage.zoom(last_conv_output, (4.57, 4.57, 1), order=1)\n    last_layer_weights_for_pred = last_layer_weights[:, pred]\n    heat_map = np.dot(upsampled_last_conv_output, last_layer_weights_for_pred) \n    flat=heat_map.flatten()\n    flat=softmax(flat)\n    \n    #plt_heatmap(cam_img,heat_map,cifar10_classes[pred])\n       \n    \n    last_layer_weights_2 = model_two.layers[-1].get_weights()[0]\n    vis_model_2 = Model(inputs=model_two.input, outputs=(model_two.layers[layer_two].output,model_two.layers[-1].output))\n    \n    with tf.GradientTape() as tape:\n        last_conv_output_2, pred_vec_2 = vis_model_2(preprocessed_img,training=False)\n        last_conv_output_2 = np.squeeze(last_conv_output_2)         \n        pred_2 = np.argmax(pred_vec_2)\n        \n    h = int(cam_img.shape[0]/last_conv_output_2.shape[0])\n    w = int(cam_img.shape[1]/last_conv_output_2.shape[1])\n    upsampled_last_conv_output_2 = ndimage.zoom(last_conv_output_2, (h, w, 1), order=1) \n    last_layer_weights_for_pred_2 = last_layer_weights_2[:, pred]\n    heat_map_2 = np.dot(upsampled_last_conv_output_2, last_layer_weights_for_pred_2) \n    flat_2=heat_map_2.flatten()\n    flat_2=softmax(flat_2)\n    \n  \n    \n    \n        \n    #plt_heatmap(cam_img,heat_map_2,cifar10_classes[pred_2])\n    \n    \n    \n    absolute_differences = np.abs(flat - flat_2)\n\n    mae = np.mean(absolute_differences)\n    \n        \n    \n    return mae\n\n\n\n\n\nclass Distiller_3(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n        self.indexing=0\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n       \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n        \n    \n    def train_step(self, x,y,conts):\n       \n       \n        x_numpy=x.numpy()\n       \n             \n        # retrieve the cam loss\n        caml=(cam_loss(self.teacher,self.student,-4,-3,x_numpy[0]))\n    \n      \n                                  \n         \n    \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n   \n            student_predictions = self.student(x, training=True)\n\n           \n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n     \n     \n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss + caml*10\n           \n            \n           \n                            \n        \n        \n        self.indexing=self.indexing+10\n        if self.indexing>49500:\n            self.indexing=0\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n       \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n        \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n       \n        return results,distillation_loss,caml\n\n    def test_step(self, data):\n     \n        x, y = data\n\n       \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        print('hi')\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results","metadata":{"execution":{"iopub.status.busy":"2023-11-15T10:10:30.033639Z","iopub.execute_input":"2023-11-15T10:10:30.034352Z","iopub.status.idle":"2023-11-15T10:10:30.060855Z","shell.execute_reply.started":"2023-11-15T10:10:30.034320Z","shell.execute_reply":"2023-11-15T10:10:30.059737Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"distiller_c2 = Distiller_3(student=student_c2, teacher=resnet50)\ndistiller_c2.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=3,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T10:10:35.307029Z","iopub.execute_input":"2023-11-15T10:10:35.307405Z","iopub.status.idle":"2023-11-15T10:10:35.324980Z","shell.execute_reply.started":"2023-11-15T10:10:35.307373Z","shell.execute_reply":"2023-11-15T10:10:35.324068Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"conts=1\nepochs = 10\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n\n    \n\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n       \n       \n\n        history,l1,l2=distiller_c2.train_step(x_batch_train,y_batch_train,conts)\n        conts+=1\n    print(conts)\n    tf.print(l1)\n    print(l2)\n    for key, value in history.items():\n        print(key, tf.print(value))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T10:10:38.293389Z","iopub.execute_input":"2023-11-15T10:10:38.293760Z","iopub.status.idle":"2023-11-15T11:48:58.491396Z","shell.execute_reply.started":"2023-11-15T10:10:38.293730Z","shell.execute_reply":"2023-11-15T11:48:58.490421Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\nStart of epoch 0\n1564\n5.21923399\n0.0019018062\n0.41108\naccuracy None\n0.977855384\nstudent_loss None\n5.21923399\ndistillation_loss None\n\nStart of epoch 1\n3127\n5.76990271\n0.0018867933\n0.46929\naccuracy None\n1.28827631\nstudent_loss None\n5.76990271\ndistillation_loss None\n\nStart of epoch 2\n4690\n6.55854082\n0.0009849127\n0.50498\naccuracy None\n1.15404582\nstudent_loss None\n6.55854082\ndistillation_loss None\n\nStart of epoch 3\n6253\n7.41599274\n0.001953125\n0.530995\naccuracy None\n2.23063707\nstudent_loss None\n7.41599274\ndistillation_loss None\n\nStart of epoch 4\n7816\n6.38461065\n0.0019531248\n0.551016\naccuracy None\n1.61445296\nstudent_loss None\n6.38461065\ndistillation_loss None\n\nStart of epoch 5\n9379\n4.75229597\n0.0019531242\n0.566683352\naccuracy None\n0.568609059\nstudent_loss None\n4.75229597\ndistillation_loss None\n\nStart of epoch 6\n10942\n5.71874142\n0.0019531203\n0.579668581\naccuracy None\n1.49964952\nstudent_loss None\n5.71874142\ndistillation_loss None\n\nStart of epoch 7\n12505\n4.3732934\n0.0019503656\n0.591137528\naccuracy None\n0.67475605\nstudent_loss None\n4.3732934\ndistillation_loss None\n\nStart of epoch 8\n14068\n4.93508\n0.001953125\n0.601222217\naccuracy None\n0.972898066\nstudent_loss None\n4.93508\ndistillation_loss None\n\nStart of epoch 9\n15631\n6.31185675\n0.001953125\n0.609912\naccuracy None\n2.30702209\nstudent_loss None\n6.31185675\ndistillation_loss None\n","output_type":"stream"}]},{"cell_type":"code","source":"distiller_c2.evaluate(test_X,test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T11:49:06.282439Z","iopub.execute_input":"2023-11-15T11:49:06.283268Z","iopub.status.idle":"2023-11-15T11:49:07.714742Z","shell.execute_reply.started":"2023-11-15T11:49:06.283233Z","shell.execute_reply":"2023-11-15T11:49:07.713943Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"hi\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6415 - student_loss: 1.3458\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[0.6414999961853027, 0.9571666717529297]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Distillation with cam loss (sigmoid,mae)","metadata":{}},{"cell_type":"code","source":"#proportionality maintained student\nstudent_c3= keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10),\n    ],\n    name=\"student_c3\",\n)\n\nstudent_c3.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T11:51:55.449078Z","iopub.execute_input":"2023-11-15T11:51:55.449950Z","iopub.status.idle":"2023-11-15T11:51:55.544601Z","shell.execute_reply.started":"2023-11-15T11:51:55.449913Z","shell.execute_reply":"2023-11-15T11:51:55.543705Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Model: \"student_c3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_10 (Conv2D)          (None, 16, 16, 128)       3584      \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n max_pooling2d_6 (MaxPoolin  (None, 16, 16, 128)       0         \n g2D)                                                            \n                                                                 \n conv2d_11 (Conv2D)          (None, 8, 8, 64)          73792     \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n max_pooling2d_7 (MaxPoolin  (None, 8, 8, 64)          0         \n g2D)                                                            \n                                                                 \n conv2d_12 (Conv2D)          (None, 4, 4, 32)          18464     \n                                                                 \n global_average_pooling2d_4  (None, 32)                0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dense_3 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 96170 (375.66 KB)\nTrainable params: 96170 (375.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef plt_heatmap(img,heatmap,title):\n    \n    fig,ax= plt.subplots()\n    ax.imshow(img)\n    #ax.imshow(heatmap, cmap='jet', alpha=0.5)\n    ax.set_title(title) \n    plt.show()\n\n\n\ndef cam_loss(model_one,model_two,layer_one,layer_two,cam_img): \n    cifar10_classes = [\n        \n        'Airplane',\n        'Automobile',\n        'Bird',\n        'Cat',\n        'Deer',\n        'Dog',\n        'Frog',\n        'Horse',\n        'Ship',\n        'Truck'\n    ]\n    \n    img_tensor = np.expand_dims(cam_img, axis=0)\n    preprocessed_img = preprocess_input(img_tensor)\n    last_layer_weights = model_one.layers[-1].get_weights()[0]\n    vis_model = Model(inputs=model_one.input, outputs=(model_one.layers[layer_one].output,model_one.layers[-1].output))\n    with tf.GradientTape() as tape:\n        last_conv_output, pred_vec = vis_model(preprocessed_img,training=False)\n      \n        last_conv_output = np.squeeze(last_conv_output) \n      \n        pred = np.argmax(pred_vec)\n   \n    upsampled_last_conv_output = ndimage.zoom(last_conv_output, (4.57, 4.57, 1), order=1)\n    last_layer_weights_for_pred = last_layer_weights[:, pred]\n    heat_map = np.dot(upsampled_last_conv_output, last_layer_weights_for_pred) \n    flat=heat_map.flatten()\n    flat=sigmoid(flat)\n    \n    #plt_heatmap(cam_img,heat_map,cifar10_classes[pred])\n       \n    \n    last_layer_weights_2 = model_two.layers[-1].get_weights()[0]\n    vis_model_2 = Model(inputs=model_two.input, outputs=(model_two.layers[layer_two].output,model_two.layers[-1].output))\n    \n    with tf.GradientTape() as tape:\n        last_conv_output_2, pred_vec_2 = vis_model_2(preprocessed_img,training=False)\n        last_conv_output_2 = np.squeeze(last_conv_output_2)         \n        pred_2 = np.argmax(pred_vec_2)\n        \n    h = int(cam_img.shape[0]/last_conv_output_2.shape[0])\n    w = int(cam_img.shape[1]/last_conv_output_2.shape[1])\n    upsampled_last_conv_output_2 = ndimage.zoom(last_conv_output_2, (h, w, 1), order=1) \n    last_layer_weights_for_pred_2 = last_layer_weights_2[:, pred]\n    heat_map_2 = np.dot(upsampled_last_conv_output_2, last_layer_weights_for_pred_2) \n    flat_2=heat_map_2.flatten()\n    flat_2=sigmoid(flat_2)\n    \n  \n    \n    \n        \n    #plt_heatmap(cam_img,heat_map_2,cifar10_classes[pred_2])\n    \n    \n    \n    absolute_differences = np.abs(flat - flat_2)\n\n    mae = np.mean(absolute_differences)\n    \n        \n    \n    return mae\n\n\n\n\n\nclass Distiller_4(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n        self.indexing=0\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n       \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n        \n    \n    def train_step(self, x,y,conts):\n       \n       \n        x_numpy=x.numpy()\n       \n             \n        # retrieve the cam loss\n        caml=(cam_loss(self.teacher,self.student,-4,-3,x_numpy[0]))\n    \n      \n                                  \n         \n    \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n   \n            student_predictions = self.student(x, training=True)\n\n           \n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n     \n     \n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss + caml\n           \n           \n           \n                            \n        \n        \n        self.indexing=self.indexing+10\n        if self.indexing>49500:\n            self.indexing=0\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n       \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n        \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n       \n        return results,distillation_loss,caml\n\n    def test_step(self, data):\n     \n        x, y = data\n\n       \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        print('hi')\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results","metadata":{"execution":{"iopub.status.busy":"2023-11-15T11:51:59.351841Z","iopub.execute_input":"2023-11-15T11:51:59.352978Z","iopub.status.idle":"2023-11-15T11:51:59.383724Z","shell.execute_reply.started":"2023-11-15T11:51:59.352941Z","shell.execute_reply":"2023-11-15T11:51:59.382645Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"distiller_c3 = Distiller_4(student=student_c3, teacher=resnet50)\ndistiller_c3.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=3,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T11:52:05.343917Z","iopub.execute_input":"2023-11-15T11:52:05.344254Z","iopub.status.idle":"2023-11-15T11:52:05.361025Z","shell.execute_reply.started":"2023-11-15T11:52:05.344227Z","shell.execute_reply":"2023-11-15T11:52:05.360103Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"conts=1\nepochs = 10\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n\n    \n\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n       \n       \n\n        history,l1,l2=distiller_c3.train_step(x_batch_train,y_batch_train,conts)\n        conts+=1\n    print(conts)\n    tf.print(l1)\n    print(l2)\n    for key, value in history.items():\n        print(key, tf.print(value))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T11:52:08.306308Z","iopub.execute_input":"2023-11-15T11:52:08.306681Z","iopub.status.idle":"2023-11-15T13:30:19.180559Z","shell.execute_reply.started":"2023-11-15T11:52:08.306650Z","shell.execute_reply":"2023-11-15T13:30:19.179641Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\nStart of epoch 0\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1672965084.py:2: RuntimeWarning: overflow encountered in exp\n  return 1 / (1 + np.exp(-x))\n","output_type":"stream"},{"name":"stdout","text":"1564\n7.32344341\n0.31949922\n0.40366\naccuracy None\n1.23258853\nstudent_loss None\n7.32344341\ndistillation_loss None\n\nStart of epoch 1\n3127\n6.26005077\n0.72482526\n0.46428\naccuracy None\n1.19808292\nstudent_loss None\n6.26005077\ndistillation_loss None\n\nStart of epoch 2\n4690\n6.33762503\n0.42104483\n0.500826657\naccuracy None\n1.90962744\nstudent_loss None\n6.33762503\ndistillation_loss None\n\nStart of epoch 3\n6253\n5.38203\n0.33154404\n0.526075\naccuracy None\n1.34064531\nstudent_loss None\n5.38203\ndistillation_loss None\n\nStart of epoch 4\n7816\n2.61925936\n0.25167355\n0.545916\naccuracy None\n0.68687582\nstudent_loss None\n2.61925936\ndistillation_loss None\n\nStart of epoch 5\n9379\n4.81336117\n0.32036978\n0.562433362\naccuracy None\n1.46040189\nstudent_loss None\n4.81336117\ndistillation_loss None\n\nStart of epoch 6\n10942\n5.61103725\n0.09207921\n0.57557714\naccuracy None\n1.12619174\nstudent_loss None\n5.61103725\ndistillation_loss None\n\nStart of epoch 7\n12505\n3.83559346\n0.12265824\n0.58706\naccuracy None\n0.568683147\nstudent_loss None\n3.83559346\ndistillation_loss None\n\nStart of epoch 8\n14068\n6.7305069\n0.22813751\n0.59706\naccuracy None\n1.71200252\nstudent_loss None\n6.7305069\ndistillation_loss None\n\nStart of epoch 9\n15631\n5.26101208\n0.5858985\n0.605776\naccuracy None\n1.1954844\nstudent_loss None\n5.26101208\ndistillation_loss None\n","output_type":"stream"}]},{"cell_type":"code","source":"distiller_c3.evaluate(test_X,test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:33:24.219719Z","iopub.execute_input":"2023-11-15T13:33:24.220614Z","iopub.status.idle":"2023-11-15T13:33:25.656709Z","shell.execute_reply.started":"2023-11-15T13:33:24.220556Z","shell.execute_reply":"2023-11-15T13:33:25.655792Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"hi\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6572 - student_loss: 1.2793\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[0.6571999788284302, 1.1914244890213013]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Distillation with cam loss (mae,random 5 samples)","metadata":{}},{"cell_type":"code","source":"#proportionality maintained student\nstudent_c4= keras.Sequential(\n    [\n        keras.Input(shape=(32, 32, 3)),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n        layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.GlobalAveragePooling2D(),\n        layers.Dense(10),\n    ],\n    name=\"student_c4\",\n)\n\nstudent_c4.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:12.684897Z","iopub.execute_input":"2023-11-15T13:36:12.685795Z","iopub.status.idle":"2023-11-15T13:36:12.790866Z","shell.execute_reply.started":"2023-11-15T13:36:12.685747Z","shell.execute_reply":"2023-11-15T13:36:12.789793Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Model: \"student_c4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_13 (Conv2D)          (None, 16, 16, 128)       3584      \n                                                                 \n leaky_re_lu_8 (LeakyReLU)   (None, 16, 16, 128)       0         \n                                                                 \n max_pooling2d_8 (MaxPoolin  (None, 16, 16, 128)       0         \n g2D)                                                            \n                                                                 \n conv2d_14 (Conv2D)          (None, 8, 8, 64)          73792     \n                                                                 \n leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 64)          0         \n                                                                 \n max_pooling2d_9 (MaxPoolin  (None, 8, 8, 64)          0         \n g2D)                                                            \n                                                                 \n conv2d_15 (Conv2D)          (None, 4, 4, 32)          18464     \n                                                                 \n global_average_pooling2d_5  (None, 32)                0         \n  (GlobalAveragePooling2D)                                       \n                                                                 \n dense_4 (Dense)             (None, 10)                330       \n                                                                 \n=================================================================\nTotal params: 96170 (375.66 KB)\nTrainable params: 96170 (375.66 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def plt_heatmap(img,heatmap,title):\n    \n    fig,ax= plt.subplots()\n    ax.imshow(img)\n    #ax.imshow(heatmap, cmap='jet', alpha=0.5)\n    ax.set_title(title) \n    plt.show()\n\n\n\ndef cam_loss(model_one,model_two,layer_one,layer_two,cam_img): \n    cifar10_classes = [\n        \n        'Airplane',\n        'Automobile',\n        'Bird',\n        'Cat',\n        'Deer',\n        'Dog',\n        'Frog',\n        'Horse',\n        'Ship',\n        'Truck'\n    ]\n    \n    img_tensor = np.expand_dims(cam_img, axis=0)\n    preprocessed_img = preprocess_input(img_tensor)\n    last_layer_weights = model_one.layers[-1].get_weights()[0]\n    vis_model = Model(inputs=model_one.input, outputs=(model_one.layers[layer_one].output,model_one.layers[-1].output))\n    with tf.GradientTape() as tape:\n        last_conv_output, pred_vec = vis_model(preprocessed_img,training=False)\n      \n        last_conv_output = np.squeeze(last_conv_output) \n      \n        pred = np.argmax(pred_vec)\n   \n    upsampled_last_conv_output = ndimage.zoom(last_conv_output, (4.57, 4.57, 1), order=1)\n    last_layer_weights_for_pred = last_layer_weights[:, pred]\n    heat_map = np.dot(upsampled_last_conv_output, last_layer_weights_for_pred) \n    flat=heat_map.flatten()\n    \n    #plt_heatmap(cam_img,heat_map,cifar10_classes[pred])\n    \n\n    \n    \n    last_layer_weights_2 = model_two.layers[-1].get_weights()[0]\n    vis_model_2 = Model(inputs=model_two.input, outputs=(model_two.layers[layer_two].output,model_two.layers[-1].output))\n    \n    with tf.GradientTape() as tape:\n        last_conv_output_2, pred_vec_2 = vis_model_2(preprocessed_img,training=False)\n        last_conv_output_2 = np.squeeze(last_conv_output_2)         \n        pred_2 = np.argmax(pred_vec_2)\n        \n    h = int(cam_img.shape[0]/last_conv_output_2.shape[0])\n    w = int(cam_img.shape[1]/last_conv_output_2.shape[1])\n    upsampled_last_conv_output_2 = ndimage.zoom(last_conv_output_2, (h, w, 1), order=1) \n    last_layer_weights_for_pred_2 = last_layer_weights_2[:, pred]\n    heat_map_2 = np.dot(upsampled_last_conv_output_2, last_layer_weights_for_pred_2) \n    flat_2=heat_map_2.flatten()\n    \n    #print(heat_map_2[0])\n    \n    #plt_heatmap(cam_img,heat_map_2,cifar10_classes[pred_2])\n    \n    \n    \n    absolute_differences = np.abs(flat - flat_2)\n\n\n    mae = np.mean(absolute_differences)\n    \n    \n    \n    \n    return mae\n\n\n\n\n\nclass Distiller_5(keras.Model):\n    def __init__(self, student, teacher):\n        super().__init__()\n        self.teacher = teacher\n        self.student = student\n        self.indexing=0\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n       \n        super().compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n        \n    \n    def train_step(self, x,y,conts):\n       \n       \n        x_numpy=x.numpy()\n       \n        \n        sum=0\n        \n        leng=int(x_numpy.size/x_numpy[0].size)\n        ind_one=0\n        for i in range(leng): \n            if ind_one >= leng:\n                break\n            sum=sum+(cam_loss(self.teacher,self.student,-4,-3,x_numpy[ind_one]))\n            ind_one+=7\n            \n        \n        caml=(sum/5)\n        \n        \n        \n                          \n         \n    \n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n\n            # Compute losses\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = (\n                self.distillation_loss_fn(\n                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n                )\n                * self.temperature**2\n            )\n     \n     \n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss + caml\n                            \n        \n        \n        self.indexing=self.indexing+10\n        if self.indexing>49500:\n            self.indexing=0\n       \n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n       \n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        \n        self.compiled_metrics.update_state(y, student_predictions)\n\n        \n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n       \n        return results\n\n    def test_step(self, data):\n     \n        x, y = data\n\n       \n        y_prediction = self.student(x, training=False)\n\n       \n        student_loss = self.student_loss_fn(y, y_prediction)\n\n      \n        self.compiled_metrics.update_state(y, y_prediction)\n\n       \n        print('hi')\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:19.045898Z","iopub.execute_input":"2023-11-15T13:36:19.046277Z","iopub.status.idle":"2023-11-15T13:36:19.074783Z","shell.execute_reply.started":"2023-11-15T13:36:19.046247Z","shell.execute_reply":"2023-11-15T13:36:19.073637Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"distiller_c4 = Distiller_5(student=student_c4, teacher=resnet50)\ndistiller_c4.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=['accuracy'],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.1,\n    temperature=3,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:26.130986Z","iopub.execute_input":"2023-11-15T13:36:26.131705Z","iopub.status.idle":"2023-11-15T13:36:26.148993Z","shell.execute_reply.started":"2023-11-15T13:36:26.131668Z","shell.execute_reply":"2023-11-15T13:36:26.148132Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"conts=1\nepochs = 10\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n\n    \n\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n       \n       \n\n        history=distiller_c4.train_step(x_batch_train,y_batch_train,conts)\n        conts+=1\n    print(conts)\n    for key, value in history.items():\n        print(key, tf.print(value))\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:31.524736Z","iopub.execute_input":"2023-11-15T13:36:31.525477Z","iopub.status.idle":"2023-11-15T19:31:16.915041Z","shell.execute_reply.started":"2023-11-15T13:36:31.525448Z","shell.execute_reply":"2023-11-15T19:31:16.914113Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"\nStart of epoch 0\n1564\n0.41358\naccuracy None\n1.09150195\nstudent_loss None\n6.02514935\ndistillation_loss None\n\nStart of epoch 1\n3127\n0.46973\naccuracy None\n2.42356873\nstudent_loss None\n9.37817383\ndistillation_loss None\n\nStart of epoch 2\n4690\n0.505046666\naccuracy None\n2.07264328\nstudent_loss None\n7.65290403\ndistillation_loss None\n\nStart of epoch 3\n6253\n0.530355\naccuracy None\n0.656530619\nstudent_loss None\n4.7881918\ndistillation_loss None\n\nStart of epoch 4\n7816\n0.550592\naccuracy None\n0.608008862\nstudent_loss None\n4.50802898\ndistillation_loss None\n\nStart of epoch 5\n9379\n0.566626668\naccuracy None\n1.24867606\nstudent_loss None\n5.79140139\ndistillation_loss None\n\nStart of epoch 6\n10942\n0.579317153\naccuracy None\n1.10779262\nstudent_loss None\n4.02469587\ndistillation_loss None\n\nStart of epoch 7\n12505\n0.59037751\naccuracy None\n1.43136072\nstudent_loss None\n5.95758629\ndistillation_loss None\n\nStart of epoch 8\n14068\n0.600113332\naccuracy None\n0.781636\nstudent_loss None\n4.29827\ndistillation_loss None\n\nStart of epoch 9\n15631\n0.608656\naccuracy None\n1.17167258\nstudent_loss None\n5.3503\ndistillation_loss None\n","output_type":"stream"}]},{"cell_type":"code","source":"distiller_c4.evaluate(test_X,test_labels)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T19:32:27.154832Z","iopub.execute_input":"2023-11-15T19:32:27.155199Z","iopub.status.idle":"2023-11-15T19:32:28.579227Z","shell.execute_reply.started":"2023-11-15T19:32:27.155167Z","shell.execute_reply":"2023-11-15T19:32:28.578332Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"hi\n313/313 [==============================] - 1s 3ms/step - accuracy: 0.6520 - student_loss: 1.2837\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"[0.6520000100135803, 0.7082465291023254]"},"metadata":{}}]}]}